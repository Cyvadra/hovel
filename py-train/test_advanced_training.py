import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from train import (
    build_model, split_data, train_model_advanced, evaluate_model,
    warmup_training, progressive_training, plot_training_history,
    WarmUpCosineDecay, ExponentialDecayWithWarmup
)

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
tf.random.set_seed(42)

def create_synthetic_data(n_samples=1000, n_features=100, n_outputs=5):
    """åˆ›å»ºåˆæˆæ•°æ®ç”¨äºæµ‹è¯•"""
    X = np.random.randn(n_samples, n_features).astype(np.float32)
    # åˆ›å»ºä¸€äº›éçº¿æ€§å…³ç³»
    Y = (np.sin(X[:, :10].sum(axis=1, keepdims=True)) + 
         np.cos(X[:, 10:20].sum(axis=1, keepdims=True)) + 
         np.random.randn(n_samples, n_outputs) * 0.1).astype(np.float32)
    return X, Y

def test_learning_rate_schedulers():
    """æµ‹è¯•å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    print("æµ‹è¯•å­¦ä¹ ç‡è°ƒåº¦å™¨...")
    
    # æµ‹è¯•ä½™å¼¦é€€ç«è°ƒåº¦å™¨
    cosine_scheduler = WarmUpCosineDecay(initial_lr=1e-3, warmup_epochs=5, total_epochs=20)
    exp_scheduler = ExponentialDecayWithWarmup(initial_lr=1e-3, warmup_epochs=3)
    
    epochs = range(20)
    cosine_lrs = [cosine_scheduler(epoch) for epoch in epochs]
    exp_lrs = [exp_scheduler(epoch) for epoch in epochs]
    
    # éªŒè¯å­¦ä¹ ç‡å˜åŒ–
    assert cosine_lrs[0] < cosine_lrs[4]  # é¢„çƒ­é˜¶æ®µåº”è¯¥å¢é•¿
    assert cosine_lrs[-1] < cosine_lrs[5]  # è¡°å‡é˜¶æ®µåº”è¯¥ä¸‹é™
    assert exp_lrs[0] < exp_lrs[2]  # é¢„çƒ­é˜¶æ®µåº”è¯¥å¢é•¿
    assert exp_lrs[-1] < exp_lrs[3]  # è¡°å‡é˜¶æ®µåº”è¯¥ä¸‹é™
    
    print("âœ“ å­¦ä¹ ç‡è°ƒåº¦å™¨æµ‹è¯•é€šè¿‡")

def test_model_building():
    """æµ‹è¯•æ¨¡å‹æ„å»º"""
    print("æµ‹è¯•æ¨¡å‹æ„å»º...")
    
    # æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨
    model_adam = build_model((100,), 5, optimizer_type='adam')
    model_adamw = build_model((100,), 5, optimizer_type='adamw')
    
    assert model_adam is not None
    assert model_adamw is not None
    assert model_adam.optimizer.__class__.__name__ == 'Adam'
    assert model_adamw.optimizer.__class__.__name__ == 'AdamW'
    
    print("âœ“ æ¨¡å‹æ„å»ºæµ‹è¯•é€šè¿‡")

def test_warmup_training():
    """æµ‹è¯•é¢„çƒ­è®­ç»ƒ"""
    print("æµ‹è¯•é¢„çƒ­è®­ç»ƒ...")
    
    # åˆ›å»ºå°è§„æ¨¡æ•°æ®
    X, Y = create_synthetic_data(n_samples=200, n_features=50)
    x_train, x_val, y_train, y_val = split_data(X, Y, test_size=0.2)
    
    # æ„å»ºæ¨¡å‹
    model = build_model((x_train.shape[1],), y_train.shape[1], optimizer_type='adamw')
    
    # è®°å½•åŸå§‹å­¦ä¹ ç‡
    original_lr = model.optimizer.learning_rate.numpy()
    
    # é¢„çƒ­è®­ç»ƒ
    warmup_history = warmup_training(model, x_train, y_train, x_val, y_val, warmup_epochs=5)
    
    # éªŒè¯å­¦ä¹ ç‡æ¢å¤
    current_lr = model.optimizer.learning_rate.numpy()
    assert abs(current_lr - original_lr) < 1e-10
    
    # éªŒè¯è®­ç»ƒå†å²
    assert 'loss' in warmup_history.history
    assert 'val_loss' in warmup_history.history
    
    print("âœ“ é¢„çƒ­è®­ç»ƒæµ‹è¯•é€šè¿‡")
    return warmup_history

def test_advanced_training():
    """æµ‹è¯•é«˜çº§è®­ç»ƒ"""
    print("æµ‹è¯•é«˜çº§è®­ç»ƒ...")
    
    # åˆ›å»ºå°è§„æ¨¡æ•°æ®
    X, Y = create_synthetic_data(n_samples=200, n_features=50)
    x_train, x_val, y_train, y_val = split_data(X, Y, test_size=0.2)
    
    # æ„å»ºæ¨¡å‹
    model = build_model((x_train.shape[1],), y_train.shape[1], optimizer_type='adamw')
    
    # æµ‹è¯•ä¸åŒå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
    strategies = ['cosine', 'exponential', 'plateau']
    
    for strategy in strategies:
        print(f"  æµ‹è¯• {strategy} è°ƒåº¦ç­–ç•¥...")
        history = train_model_advanced(
            model, x_train, y_train, x_val, y_val,
            epochs=10,
            batch_size=32,
            lr_schedule=strategy
        )
        
        # éªŒè¯è®­ç»ƒå†å²
        assert 'loss' in history.history
        assert 'val_loss' in history.history
        assert 'mae' in history.history
        assert 'mse' in history.history
    
    print("âœ“ é«˜çº§è®­ç»ƒæµ‹è¯•é€šè¿‡")

def test_progressive_training():
    """æµ‹è¯•æ¸è¿›å¼è®­ç»ƒ"""
    print("æµ‹è¯•æ¸è¿›å¼è®­ç»ƒ...")
    
    # åˆ›å»ºå°è§„æ¨¡æ•°æ®
    X, Y = create_synthetic_data(n_samples=200, n_features=50)
    x_train, x_val, y_train, y_val = split_data(X, Y, test_size=0.2)
    
    # æ„å»ºæ¨¡å‹
    model = build_model((x_train.shape[1],), y_train.shape[1], optimizer_type='adamw')
    
    # æ¸è¿›å¼è®­ç»ƒ
    training_history = progressive_training(
        model, x_train, y_train, x_val, y_val,
        batch_sizes=[16, 32],
        epochs_per_stage=5
    )
    
    # éªŒè¯è®­ç»ƒå†å²
    assert len(training_history) == 2  # ä¸¤ä¸ªé˜¶æ®µ
    
    for history in training_history:
        assert 'loss' in history.history
        assert 'val_loss' in history.history
    
    print("âœ“ æ¸è¿›å¼è®­ç»ƒæµ‹è¯•é€šè¿‡")
    return training_history

def test_evaluation():
    """æµ‹è¯•æ¨¡å‹è¯„ä¼°"""
    print("æµ‹è¯•æ¨¡å‹è¯„ä¼°...")
    
    # åˆ›å»ºå°è§„æ¨¡æ•°æ®
    X, Y = create_synthetic_data(n_samples=200, n_features=50)
    x_train, x_val, y_train, y_val = split_data(X, Y, test_size=0.2)
    
    # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
    model = build_model((x_train.shape[1],), y_train.shape[1], optimizer_type='adamw')
    history = train_model_advanced(
        model, x_train, y_train, x_val, y_val,
        epochs=5,
        batch_size=32,
        lr_schedule='cosine'
    )
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_model(model, x_val, y_val)
    
    # éªŒè¯ç»“æœ
    assert len(results) == 3  # loss, mae, mse
    assert all(isinstance(r, float) for r in results)
    assert all(r >= 0 for r in results)
    
    print("âœ“ æ¨¡å‹è¯„ä¼°æµ‹è¯•é€šè¿‡")
    return results

def test_visualization():
    """æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½"""
    print("æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½...")
    
    # åˆ›å»ºå°è§„æ¨¡æ•°æ®
    X, Y = create_synthetic_data(n_samples=200, n_features=50)
    x_train, x_val, y_train, y_val = split_data(X, Y, test_size=0.2)
    
    # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
    model = build_model((x_train.shape[1],), y_train.shape[1], optimizer_type='adamw')
    
    # é¢„çƒ­è®­ç»ƒ
    warmup_history = warmup_training(model, x_train, y_train, x_val, y_val, warmup_epochs=3)
    
    # ä¸»è®­ç»ƒ
    training_history = train_model_advanced(
        model, x_train, y_train, x_val, y_val,
        epochs=5,
        batch_size=32,
        lr_schedule='cosine'
    )
    
    # æµ‹è¯•å¯è§†åŒ–
    try:
        plot_training_history([warmup_history, training_history], "æµ‹è¯•è®­ç»ƒå†å²")
        print("âœ“ å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âš  å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œé«˜çº§è®­ç»ƒåŠŸèƒ½æµ‹è¯•...")
    print("=" * 50)
    
    try:
        test_learning_rate_schedulers()
        test_model_building()
        warmup_history = test_warmup_training()
        test_advanced_training()
        training_history = test_progressive_training()
        results = test_evaluation()
        test_visualization()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 50)
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"æœ€ç»ˆè¯„ä¼°ç»“æœ:")
        print(f"  æŸå¤±: {results[0]:.4f}")
        print(f"  MAE: {results[1]:.4f}")
        print(f"  MSE: {results[2]:.4f}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = run_all_tests()
    if success:
        print("\nâœ… é«˜çº§è®­ç»ƒç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
    else:
        print("\nâŒ è¯·æ£€æŸ¥é”™è¯¯å¹¶ä¿®å¤é—®é¢˜") 